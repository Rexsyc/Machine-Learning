#机器学习

机器学习是一类算法的总称，这些算法企图从大量历史数据中挖掘出其中隐含的规律，并用于预测或者分类，更具体的说，机器学习可以看作是寻找一个函数，输入是样本数据，输出是期望的结果，只是这个函数过于复杂，以至于不太方便形式化表达。需要注意的是，机器学习的目标是使学到的函数很好地适用于“新样本”，而不仅仅是在训练样本上表现很好。学到的函数适用于新样本的能力，称为泛化（Generalization）能力。

根据机器学习的应用，一般说来，机器学习有三种算法： 

####监督式学习（Supervised Learning， SL）

监督学习是指原始数据中既有特征值也有标签值的机器学习。用输入层的数据计算输出层的值，然后对比标签值计算误差，再通过迭代找到最佳模型参数。所谓机器学习就是指对已知数据不断迭代从而找到最佳参数的过程。

属于监督式学习的算法有：KNN、回归模型，决策树，随机森林，K邻近算法，逻辑回归、SVM等。

####无监督式学习（Unsupervised Learning， UL）

与监督式学习不同的是，无监督学习中我们没有需要预测或估计的目标变量（或标签值）。无监督式学习是用来对总体对象进行分类的。它在根据某一指标将客户分类上有广泛应用。

属于无监督式学习的算法有：关联规则，K-means聚类算法等。

####强化学习（Reinforcement Learning， RL）

这个算法可以训练程序做出某一决定。程序在某一情况下尝试所有的可能行动，记录不同行动的结果并试着找出最好的一次尝试来做决定。

属于这一类算法的有马尔可夫决策过程。



##scikit-learn模块

scikit-learn(简记sklearn)，是机器学习中一个常用的python第三方模块，里面对一些常用的机器学习方法进行了封装，在进行机器学习任务时，并不需要每个人都实现所有的算法，只需要简单的调用sklearn里的模块就可以实现大多数机器学习任务。

机器学习任务通常包括分类（Classification）和回归（Regression），常用的分类器包括SVM、KNN、贝叶斯、线性回归、逻辑回归、决策树、随机森林、xgboost、GBDT、boosting、神经网络NN。
常见的降维方法包括TF-IDF、主题模型LDA、主成分分析PCA等等

sklearn可以实现数据预处理、分类、回归、降维、模型选择等常用的机器学习算法。sklearn是基于NumPy, SciPy, matplotlib的。

NumPy python实现的开源科学计算包。它可以定义高维数组对象；矩阵计算和随机数生成等函数。

SciPy python实现的高级科学计算包。它和Numpy联系很密切，Scipy一般都是操控Numpy数组来进行科学计算，所以可以说是基于Numpy之上了。Scipy有很多子模块可以应对不同的应用，例如插值运算，优化算法、图像处理、数学统计等。

matplotlib python实现的作图包。使用matplotlib能够非常简单的可视化数据，仅需要几行代码，便可以生成直方图、功率谱、条形图、错误图、散点图等。

##pandas模块

pandas 是基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。pandas提供了大量能使我们快速便捷地处理数据的函数和方法,它是使Python成为强大而高效的数据分析环境的重要因素之一。 
Pandas 的数据结构：Pandas 主要有 Series（一维数组），DataFrame（二维数组），Panel（三维数组），Panel4D（四维数组），PanelND（更多维数组）等数据结构。其中 Series 和 DataFrame 应用的最为广泛。
Series 是一维带标签的数组，它可以包含任何数据类型。包括整数，字符串，浮点数，Python 对象等。Series 可以通过标签来定位。
DataFrame 是二维的带标签的数据结构。我们可以通过标签来定位数据。这是 NumPy 所没有的。



# 机器学习/数据挖掘流程:

数据收集-数据清洗-特征工程-数据分析-算法实现-模型评价与改进

###数据收集

**数据采集的方法**

数据收集的方法可以分成两类：

1. 直接来源：一手数据

   ```python
   一手数据的正式版定义是：通过研究者实施的调查或实验活动获得的数据。
   
   所以，要想获得一手数据，有两种方法：调查 或 实验。
   
   调查
   通过调查得到的一手数据叫做调查数据。调查数据是针对社会现象的。比如说，调查现在的经济形势、人的心理现象、工厂效率等等。
   
   调查的形式
   调查的形式分为两类：
   
   普查
   抽样
   普查是要对一个总体内部的所有个体进行调查，国家进行的人口普查就是最典型的普查形式。普查的结果是最贴近总体的真实表现的，是无偏见（Unbias）的估测。但是普查的成本太大，少有项目采用这种方式。
   
   抽样则在生活中被应用的更加广泛。由于数据分析挖掘涉及的总体数据量一般很大，如果要做普查，没有大规模的时间与金钱是几乎不可能的。所以，我们会从总体中抽取部分有代表性的个体调查，并用这部分个体的数据去反映整体，这就是抽样。
   
   调查的方法
   不管是用普查还是抽样的方法，数据采集都习惯用下面三种方式之一：
   
   自填式：填写调查问卷（电子/书面）。
   面访式：面对面采访。
   电话式：电话联络。
   具体的设计问卷的方法、面访的技巧或者电话的提问设计都在后面慢慢讲解。这里只是概括性的介绍方法。
   
   
   实验
   通过实验得到的一手数据叫做实验数据。调查数据是针对自然现象的。比如说，植物背光生长的快慢、小白鼠对食物的记忆规律等等。
   
   实验的方法
   实验方法需要研究者真正设计实验，并记录结果、整合为数据，服务于后期的数据分析与挖掘工作。
   
   实验的设计需要满足一个大原则：有实验组与对照组。实验组是只有要研究的变量发生变化的组；对照组是保持变量不变的组。这样，通过控制变量的方法，能得到观测数据。
   
   ```

2. 间接来源：二手数据

   ```python
   二手数据的正式版定义是：数据原本已经存在，是由别人收集的，使用者通过重新加工或整理得到的数据。
   
   所以，要想获得二手数据，有两种方法：系统内部采集 或 系统外部采集。
   
   系统内部采集
   系统内部采集数据是我在工作中最常见的数据采集方法。要进行数据分析的公司肯定会有自己的数据，这些数据一般会保存在数据库中，我有过接触的数据库例如Oracle与Teradata。在数据仓库中，会保存公司内部的生产数据，他们就是将公司的业务、渠道、成本、收益等生产过程数字化并固定存放在机器中。数据挖掘师可以通过SQL语言提取想要的数据表，并进行数据的收集。
   
   系统内部数据一般都与企业的生产相关，涉及到用户信息的保密与商业机密等问题。所以一般都是有项目或者有研究课题的时候才能够获取。
   
   
   系统外部采集
   系统外部采集的数据是更加宏观、更加公开的数据。这些数据大部分不是针对某一家公司自己的运营与生产情况，而是更加偏重于社会的外部环境以及行业的经济形势。
   
   下面这些都是系统外部采集的常用渠道：
   
   统计部门或政府的公开资料、统计年鉴
   调查机构、行会、经济信息中心发布的数据情报
   专业期刊
   图书
   博览会
   互联网
   系统外部采集数据的源头众多，采集方法也有很多，手工处理excel或者网络爬虫都是可选的方法。
   
   ```

   

###数据清洗

为了更好地处理数据从而得到与问题相关的特征， 在对问题建模分析以前需 要做一些预处理的工作，例如对数据的格式进行转换、清理数据集中的异常值、 纠正错误数据等    

* 错误数据

  * 保单性质,对保单性质进行处理，其中：转保为0,续保为1

* 数量级相差太大

  * 渠道  门店:15 做删除处理  直拓:4  做删除处理

  * 车辆种类  特种车二挂车:2 做删除处理   5吨及10吨以下挂车:4 做删除处理  低速载货汽车:11 做删除处理特种车一:13 做删除处理

  * 车辆用途  轻微型载货汽车:19 做删除处理  带拖挂汽车:8 做删除处理  矿山专用车:7 做删除处理

    低速货车和三轮汽车:6 做删除处理  重、中型载货汽车:2 做删除处理  其他汽车:1 做删除处理

  * NCD  i.上年出险四次:26 做删除处理  新保无记录:9 做删除处理

    6.上年发生有责死亡事故:8 做删除处理  j.上年出险五次及以上:8 做删除处理

* 2.缺失值的处理

  * 删除
    * NCD存在11条缺失值
    * 车辆用途存在1条缺失值
    * 车辆种类存在1条缺失值
  * 填充
    * 被保险人年龄,在这里空缺值填充为平均值
    * 被保险人性别,机构:0, M:1, F:2

###特征工程

```python
	特征是数据中抽取出来的对模型算法有用的信息，可以是文本或者数据。好的特征具有更强的灵活性，可以用简单的模型做训练，更可以得到优秀的结果。本文为了筛选出更好的特征，获取更好的训练数据，本文中引入特征工程。
	特征工程是使用专业背景知识和数据挖掘技术处理数据， 从数据集中提炼出真正对问题求解有用的特征，其过程包含特征提取、特征构建、特征选择等步骤。

数值统计特征
针对拥有丰富的数据但缺乏有用特征的情形， 数据挖掘技术可以从大量数据中挖掘对数据样本的多方面数值型描述。本文中对中证 800 指数数据集运用统计学知识，提取每一只股票数据的均值、方差、标准差、极差、极大值、极小值、
四分位点、缺失值个数、中位数等统计特征。并对每一只股票数据做一阶差分运算， 以构建数据样本涨幅、跌幅、增长率、下跌率等数值特征并对一阶差分后的数据做同样的数据统计。

工程实际意义特征
为了使分析更具有意义，分析结果更加真实可靠， 通过查阅相关资料，选取了一系列股票评价指标。例如： 10 日涨跌比率 ADR、 10 日涨跌比率 ADR 的均值与极差、 10 日相对强弱指标 RSI、 10 日相对强弱指标 RSI 的均值和极差、 N日乖离率 BIAS、 N 日乖离率 BIAS 的均值和极差、 N 日 RSV、 N 日 RSV 的均值和极差、 N 日 ROC、 N 日 ROC 的均值和极差、 N 日涨幅、 N 日跌幅等具有工程实际意义的工程性指标。

模型特征
由于数据集数据量较大， 在提取数值统计特征以及工程实际意义特征的过程中会伴随着信息的损失。 为了对每一只股票提取信息量足够多的特征， 本文中采用流行学习(TSNE)降维的方法。流行学习可以从高维采样数据中恢复低维流行
结构，即找到高维空间中的低维流行，并求出相应的嵌入映射，以实现维数简约。本模型中将每只股票的 2167 个数据点压缩到 10 维空间内，从而刻画数据的内在规律.
```

* 1.增加/减少特征

  * 我们对起保日期和终止日期进行分析，发现时间间隔都为 1 年，因此这两类数据对客户续保概率影响不大，我们将这两类属性删除。
  * 由于品牌和车系与新车购置价有一定的联系，通常不同品牌不同车系都会有对应的价格，因此我们选取新车购置价来进行后续的分析建模。
  * 在前面的数据清洗过程中，我们发现保单性质、续保年份以及是否续保具有强相关性，因此我们将与是否续保具有一一对应关系的保单性质以及续保年份这两条属性去掉，避免影响后续的结果分析。
  * 对于存在缺失值 70%以上的数据， 由于其缺失值过多， 可以将其认为无效属性，将其删除。已决赔款以及风险类别缺失值

* 2.特征编码

  (1)相似数据的简并

  (2)数据类型的转换

  * 投保类别   
  * 是否本省车牌 
  * 险种
  * 客户类别
  * 是否投保车损
  * 是否投保盗抢
  * 是否投保车上人员
  * 是否续保

  (3)连续属性离散化

  *  新车购置价的离散化
  * 车龄的离散化
  * 被保险人年龄的离散化
  * 三者险保额的离散化
  * 签单保费的离散化

* 3.标准化与归一化

  归一化：

  １）把数据变成(０，１)或者（1,1）之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。

  ２）把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。

  标准化：在机器学习中，我们可能要处理不同种类的资料，例如，音讯和图片上的像素值，这些资料可能是高维度的，标准化就是将训练集中某一列数值特征（假设是第i列）的值缩放成均值为0，方差为1的状态。这个方法被广泛的使用在许多机器学习算法中(例如：支持向量机、逻辑回归和类神经网络)。

  中心化：平均值为0，对标准差无要求

  归一化和标准化的区别：归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。

  标准化和中心化的区别：标准化是原始分数减去平均数然后除以标准差，中心化是原始分数减去平均数。 所以一般流程为先中心化再标准化。

* 4.降维

  **缺失值比率 (Missing Values Ratio)**：该方法的是基于包含太多缺失值的数据列包含有用信息的可能性较少。因此，可以将数据列缺失值大于某个阈值的列去掉。阈值越高，降维方法更为积极，即降维越少。

  **随机森林/组合树 (Random Forests)：**组合决策树通常又被成为随机森林，它在进行特征选择与构建有效的分类器时非常有用。一种常用的降维方法是对目标属性产生许多巨大的树，然后根据对每个属性的统计结果找到信息量最大的特征子集。例如，我们能够对一个非常巨大的数据集生成层次非常浅的树，每颗树只训练一小部分属性。如果一个属性经常成为最佳分裂属性，那么它很有可能是需要保留的信息特征。对随机森林数据属性的统计评分会向我们揭示与其它属性相比，哪个属性才是预测能力最好的属性。

  **低方差滤波 (Low Variance Filter)：**与缺失值比率进行数据降维方法相似，该方法假设数据列变化非常小的列包含的信息量少。因此，所有的数据列方差小的列被移除。需要注意的一点是：方差与数据范围相关的，因此在采用该方法前需要对数据做归一化处理。 

  **高相关滤波 (High Correlation Filter)：**高相关滤波认为当两列数据变化趋势相似时，它们包含的信息也相似。这样，使用相似列中的一列就可以满足机器学习模型。对于数值列之间的相似性通过计算相关系数来表示，对于数据类型为类别型的相关系数可以通过计算皮尔逊卡方值来表示。相关系数大于某个阈值的两列只保留一列。同样要注意的是：相关系数对范围敏感，所以在计算之前也需要对数据进行归一化处理。

  **主成分分析 (PCA)：**主成分分析是一个统计过程，该过程通过正交变换将原始的 n 维数据集变换到一个新的被称做主成分的数据集中。变换后的结果中，第一个主成分具有最大的方差值，每个后续的成分在与前述主成分正交条件限制下具有最大方差。降维时仅保存前 m(m < n) 个主成分即可保持最大的数据信息量。

  **因子分析（FA） ：**通过研究众多变量之间的内部依赖关系，探求观测数据中的基本结构，并用少数几个假想变量来表示其基本的数据结构。这几个假想变量能够反映原来众多变量的主要信息。原始的变量是可观测的显在变量，而假想变量是不可观测的潜在变量，称为因子。

  因子分析又存在两个方向，一个是探索性因子分析。另一个是验证性因子分析。探索性因子分析是不确定一堆自变量背后有几个因子，我们通过这种方法试图寻找到这几个因子。而验证性因子分析是已经假设自变量背后有几个因子，试图通过这种方法去验证一下这种假设是否正确。验证性因子分析又和结构方程模型有很大关系。

  **反向特征消除 (Backward Feature Elimination)：**在该方法中，所有分类算法先用 n 个特征进行训练。每次降维操作，采用 n-1 个特征对分类器训练 n 次，得到新的 n 个分类器。将新分类器中错分率变化最小的分类器所用的 n-1 维特征作为降维后的特征集。不断的对该过程进行迭代，即可得到降维后的结果。第k 次迭代过程中得到的是 n-k 维特征分类器。通过选择最大的错误容忍率，我们可以得到在选择分类器上达到指定分类性能最小需要多少个特征。

  **前向特征构造 (Forward Feature Construction)****：**前向特征构建是反向特征消除的反过程。在前向特征过程中，我们从 1 个特征开始，每次训练添加一个让分类器性能提升最大的特征。前向特征构造和反向特征消除都十分耗时。它们通常用于输入维数已经相对较低的数据集。 

  **数据降维除了上述提到的几种，还包括：**

  随机投影(Random Projections)；
  非负矩阵分解(N0n-negative Matrix Factorization)；
  自动编码(Auto-encoders)；
  卡方检测与信息增益(Chi-square and information gain)；
  多维标定(Multidimensional Scaling)；

###数据分析

数据分析是指用适当的统计分析方法对收集来的大量数据进行分析，提取有用信息和形成结论而对数据加以详细研究和概括总结的过程。这一过程也是质量管理体系的支持过程。在实用中，数据分析可帮助人们作出判断，以便采取适当行动。
     在统计学领域，有些人将数据分析划分为描述性统计分析、探索性数据分析以及验证性数据分析；

####1、描述性数据分析（Descriptive Statistics） 

  描述性统计，是指运用制表和分类，图形以及计算概括性数据来描述数据特征的各项活动。描述性统计分析要对调查总体所有变量的有关数据进行统计性描述，主要包括数据的频数分析、集中趋势分析、离散程度分析、分布以及一些基本的统计图形。
         ①数据的频数分析。在数据的预处理部分，利用频数分析和交叉频数分析可以检验异常值。
         ②数据的集中趋势分析。用来反映数据的一般水平，常用的指标有平均值、中位数和众数等。
         ③数据的离散程度分析。主要是用来反映数据之间的差异程度，常用的指标有方差和标准差。
        ④数据的分布。在统计分析中，通常要假设样本所属总体的分布属于正态分布，因此需要用偏度和峰度两个指标来检查样本数据是否符合正态分布。
       ⑤绘制统计图。用图形的形式来表达数据，比用文字表达更清晰、更简明。在SPSS软件里，可以很容易地绘制各个变量的统计图形，包括条形图、饼图和折线图等。

####2、探索性数据分析（Explorratory Data Analysis）

  探索性数据分析是指为了形成值得假设的检验而对数据进行分析的一种方法，当数据分析者不清楚数据中包含的什么模型或者隐含的什么关系时，尝试各种方法来探索发现数据中可能存在的关系。这是对传统统计学假设检验手段的补充。该方法由美国著名统计学家约翰·图基(John Tukey)命名。探索性数据分析侧重于在数据之中发现新的特征，发现数据的性质特征，为后面的分析提供研究价值。当数据分析人员拿到初步的数据时，可通过探索性的数据分析，解决一些问题，如：
         a.发现错误和丢失的数据;
         b.绘制数据的底层结构;
         c.确定最重要的变量;
         d.列出异常和异常值;
         e.测试与具体模型相关的假设/检查假设;
         f.建立一个简约的模型（可用于用最小预测变量来解释数据的模型）;
         g.估计参数并计算相关的置信区间或误差范围。

####3、验证性数据分析（Confirmatory Data Analysis）

验证性数据分析侧重于已有假设的证实或证伪。即数据分析者已经有事先假设的关系模型，要通过数据分析来对其假设模型进行验证。

###算法实现

### 模型评价与改进

